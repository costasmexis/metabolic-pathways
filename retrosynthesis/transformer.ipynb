{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "# ignore some deprecation warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load main data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...</td>\n",
       "      <td>CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1</td>\n",
       "      <td>COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C</td>\n",
       "      <td>CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...   \n",
       "1      Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1    \n",
       "2    CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C    \n",
       "\n",
       "                                              target  \n",
       "0   CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...  \n",
       "1      COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1  \n",
       "2   CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('retrosynthesis-all', header=None)\n",
    "df['source'] = df[0].apply(lambda x: x.split('>>')[0])\n",
    "df['target'] = df[0].apply(lambda x: x.split('>>')[1])\n",
    "df.drop(0, axis=1, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Vocabulary: How is it generated?\n",
    "\n",
    "The model's Vocabulary handles the transformation of SMILES strings into a sequence of tokens. Tokens are the pre-defined lowest and indivisible unit of string text. In Natural Language Processing (NLP), tokens are typically defined on the word or character level. The level of tokenization dictates *what* the model can output, e.g., if tokenization on the character level is used, then the model outputs individual characters.\n",
    "\n",
    "For generative SMILES models, tokenization is performed on the character level where each token *loosely* maps to a unique atom type (brackets, \"(\" for example indicate branching and thus, do not map to an atom but rather gives connectivity information).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, Vocabulary, create_vocabulary\n",
    "\n",
    "tk = SMILESTokenizer()\n",
    "vocabulary = Vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'Br']\n"
     ]
    }
   ],
   "source": [
    "# smi_sample = df['source'].iloc[123]\n",
    "smi_sample = 'CCBr'\n",
    "print(tk.tokenize(smi_sample, with_begin_and_end=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in a natural language like english where one's vocabulary controls what sentences can be formulated, a molecular generative model's vocabulary controls what kind of atoms can be proposed - sentences in this context are molecules\n",
    "\n",
    "In order for the token representations of SMILES sequences to be passed into a machine learning model, they must be transformed into a numerical representation. This is done in the Vocabulary class where each unique token is mapped to a numerical index. This is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 unique tokens in the vocabulary.\n",
      "\n",
      "The unique tokens are: \n",
      "['$', '^', ' ', '#', '(', ')', '-', '.', '/', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH3-]', '[Br-]', '[C-]', '[C@@H]', '[C@@]', '[C@H]', '[C@]', '[Cl+3]', '[Cl-]', '[Cu]', '[Fe]', '[I+]', '[K]', '[Li]', '[Mg+]', '[Mg]', '[N+]', '[N-]', '[N@+]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[NH4+]', '[O-]', '[OH-]', '[P+]', '[PH2]', '[PH4]', '[PH]', '[Pd]', '[Pt]', '[S+]', '[S-]', '[S@@]', '[S@]', '[SH]', '[Se]', '[SiH2]', '[SiH]', '[Si]', '[SnH]', '[Sn]', '[Zn+]', '[Zn]', '[n+]', '[n-]', '[nH]', '[s+]', '[se]', '\\\\', 'c', 'n', 'o', 's']\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary using all SMILES in df\n",
    "smiles_dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "smiles_dataset = np.unique(smiles_dataset).tolist()\n",
    "\n",
    "vocabulary = create_vocabulary(smiles_list=smiles_dataset, tokenizer=tk)\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'The unique tokens are: \\n{vocabulary.tokens()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCBr\n",
      "['C', 'C', 'Br']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21., 21., 20.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(smi_sample)\n",
    "tokenized_smi_sample = tk.tokenize(smi_sample, with_begin_and_end=False)\n",
    "print(tokenized_smi_sample)\n",
    "\n",
    "vocabulary.encode(tokenized_smi_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Î¤he `encode` method in the `Vocabulary` class takes a list of tokens and returns its numerical indices the indices are what we expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes *how* the numerical representation of tokens are transformed into an input vector known as the embedding that will act as the input to the RNN.\n",
    "\n",
    "An Embedding Layer is essentially a look-up table. In the constructor above, `num_embeddings` refers to the Vocabulary size. \n",
    "\n",
    "`num_embeddings` denotes how many vectors to initialize. \n",
    "\n",
    "Since we have n unique tokens, we need _ different vectors: 1 for each unique token. This is why `num_embeddings` is _ in this example. `embedding_dim` denotes the dimension of the embedding vector. 5 is arbitrarily chosen here just for easy visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an \"Embedding layer\"\n",
    "EMBEDDING_DIM = 5\n",
    "NUM_EMBEDDING = len(vocabulary)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=NUM_EMBEDDING,\n",
    "                               embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# only 1 layer of LSTM cells is initialized here for the sake of illustration\n",
    "# input_size = 5 because we previously defined the \"embedding_dim\" of the Embedding layer to be 5\n",
    "# hidden_size = 5 is arbitrarily chosen for easy visualization\n",
    "recurrent_layer = nn.LSTM(input_size=EMBEDDING_DIM,\n",
    "                          hidden_size=5,\n",
    "                          num_layers=1,\n",
    "                          dropout=0,\n",
    "                          batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical indices of bromoethane:\n",
      " tensor([[21, 21, 20]])\n",
      "\n",
      "Embedding:\n",
      " tensor([[[-0.3277,  0.9600, -0.4915, -0.2581, -1.1825],\n",
      "         [-0.3277,  0.9600, -0.4915, -0.2581, -1.1825],\n",
      "         [ 1.1409, -0.3243, -1.1002,  1.4983, -0.0269]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# get the numerical indices of bromoethane\n",
    "numerical_indices_smi_sample = torch.LongTensor([vocabulary.encode(tokenized_smi_sample).astype(int)])\n",
    "print(f\"Numerical indices of bromoethane:\\n {numerical_indices_smi_sample}\\n\")\n",
    "\n",
    "embedding = embedding_layer(numerical_indices_smi_sample)\n",
    "print(f\"Embedding:\\n {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5])\n",
      "torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)\n",
    "\n",
    "# let's run the embedding through the recurrent layer\n",
    "rnn_output, (hidden_state, cell_state) = recurrent_layer(embedding)\n",
    "\n",
    "print(rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 86])\n",
      "Softmax shape: torch.Size([1, 3, 86])\n",
      "Log-Softmax shape: torch.Size([1, 3, 86])\n",
      "Original SMILES string: CCBr\n",
      "\n",
      "The unique tokens are: \n",
      "['$', '^', ' ', '#', '(', ')', '-', '.', '/', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH3-]', '[Br-]', '[C-]', '[C@@H]', '[C@@]', '[C@H]', '[C@]', '[Cl+3]', '[Cl-]', '[Cu]', '[Fe]', '[I+]', '[K]', '[Li]', '[Mg+]', '[Mg]', '[N+]', '[N-]', '[N@+]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[NH4+]', '[O-]', '[OH-]', '[P+]', '[PH2]', '[PH4]', '[PH]', '[Pd]', '[Pt]', '[S+]', '[S-]', '[S@@]', '[S@]', '[SH]', '[Se]', '[SiH2]', '[SiH]', '[Si]', '[SnH]', '[Sn]', '[Zn+]', '[Zn]', '[n+]', '[n-]', '[nH]', '[s+]', '[se]', '\\\\', 'c', 'n', 'o', 's']\n",
      "\n",
      "At time step 1, the generative model proposes [n+] as the most probable token and the correct token is C\n",
      "At time step 2, the generative model proposes [Cu] as the most probable token and the correct token is C\n",
      "At time step 3, the generative model proposes [n+] as the most probable token and the correct token is B\n"
     ]
    }
   ],
   "source": [
    "# initialize the linear layer\n",
    "# in_features = 5 as that is the hidden_size defined in the recurrent layer above\n",
    "# out_features = 20 as that is the size of the Vocabulary\n",
    "linear_layer = nn.Linear(in_features=5,\n",
    "                         out_features=NUM_EMBEDDING)\n",
    "\n",
    "linear_output = linear_layer(rnn_output)\n",
    "\n",
    "# verify the shape of the linear output is what we expect:\n",
    "# (batch size) x (sequence length) x (vocabulary size)\n",
    "print(linear_output.shape)\n",
    "\n",
    "# let's first show the normal softmax output\n",
    "# recall the output from the linear layer has dimensions: (batch size) x (sequence length) x (vocabulary size)\n",
    "# therefore, dim=2 because we want to compute the softmax over the vocabulary to obtain a probability for each token\n",
    "softmax = linear_output.softmax(dim=2)\n",
    "print(f\"Softmax shape: {softmax.shape}\")\n",
    "\n",
    "# let's now show the log-softmax output\n",
    "log_softmax = linear_output.log_softmax(dim=2)\n",
    "print(f\"Log-Softmax shape: {log_softmax.shape}\")\n",
    "\n",
    "# log-softmax to token probabilities\n",
    "# recall our original SMILES \n",
    "print(f\"Original SMILES string: {smi_sample}\\n\")\n",
    "\n",
    "# recall our vocabulary\n",
    "print(f\"The unique tokens are: \\n{vocabulary.tokens()}\\n\")\n",
    "\n",
    "# we now extract the max value in each tensor of the log-softmax output above and the corresponding token\n",
    "most_probable_tokens = log_softmax.argmax(dim=2).flatten().tolist()\n",
    "for idx, (correct_token, most_probable_token) in enumerate(zip(smi_sample, most_probable_tokens)):\n",
    "    print(f\"At time step {idx+1}, the generative model proposes {vocabulary.tokens()[most_probable_token]} as the most probable token and the correct token is {correct_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does The Model Learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the token indices we would want our model to predict:\n",
      "tensor([[21, 21, 20]])\n",
      "\n",
      "Recall the log-softmax output:\n",
      "torch.Size([1, 3, 86])\n",
      "\n",
      "We will transpose the log-softmax tensor to have shape (batch size) x (vocabulary) x (sequence length):\n",
      "torch.Size([1, 86, 3])\n",
      "\n",
      "The output tensor from negative log-likelihood is:\n",
      "tensor([[4.1875, 4.1391, 4.5312]], grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the loss\n",
    "loss = torch.nn.NLLLoss(reduction='none')\n",
    "# recall the correct token indices of \"CCBr\" which was our original SMILES\n",
    "print(f\"These are the token indices we would want our model to predict:\\n{numerical_indices_smi_sample}\\n\")\n",
    "print(f\"Recall the log-softmax output:\\n{log_softmax.shape}\\n\")\n",
    "print(f\"We will transpose the log-softmax tensor to have shape \\\n",
    "(batch size) x (vocabulary) x (sequence length):\\n{log_softmax.transpose(1,2).shape}\\n\")\n",
    "\n",
    "print(f\"The output tensor from negative log-likelihood is:\\n{loss(log_softmax.transpose(1, 2), numerical_indices_smi_sample)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During backpropagation, we take the sum of the `Negative Log-Likehood` tensor to \"summarize\" the total loss associated with a given SMILES sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.0889], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(log_softmax.transpose(1, 2), numerical_indices_smi_sample).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset that takes a file containing \\n separated SMILES.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smiles_list : list, vocabulary : Vocabulary,\n",
    "                 tokenizer : SMILESTokenizer) -> None:\n",
    "        self._vocabulary  = vocabulary\n",
    "        self._tokenizer   = tokenizer\n",
    "        self._smiles_list = list(smiles_list)\n",
    "\n",
    "    def __getitem__(self, i : int) -> torch.Tensor:\n",
    "        smi     = self._smiles_list[i]\n",
    "        tokens  = self._tokenizer.tokenize(smi)\n",
    "        encoded = self._vocabulary.encode(tokens)\n",
    "        return torch.tensor(encoded, dtype=torch.long)  # pylint: disable=E1102\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._smiles_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(encoded_seqs : list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts a list of encoded sequences into a padded tensor.\n",
    "        \"\"\"\n",
    "        max_length   = max([seq.size(0) for seq in encoded_seqs])\n",
    "        collated_arr = torch.zeros(len(encoded_seqs),\n",
    "                                   max_length,\n",
    "                                   dtype=torch.long)  # padded with zeros\n",
    "        for i, seq in enumerate(encoded_seqs):\n",
    "            collated_arr[i, :seq.size(0)] = seq\n",
    "        return collated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "benzene = 'c1ccccc'\n",
    "# tokenize\n",
    "benzene_tokenized = tk.tokenize(benzene)\n",
    "# encode (obtain numerical token indices)\n",
    "benzene_encoded = vocabulary.encode(benzene_tokenized)\n",
    "# transform it into a tensor for collating\n",
    "benzene_encoded = torch.tensor([benzene_encoded])\n",
    "# collate\n",
    "benzene_collated = Dataset.collate_fn(benzene_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "class SmilesLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocabulary: Vocabulary, tokenizer: SMILESTokenizer,\n",
    "                model_parameters: Union[dict, None]=None) -> None:\n",
    "        \n",
    "        self.vocabulary = vocabulary\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = len(self.vocabulary)\n",
    "        \n",
    "        self._layer_size           = model_parameters['layer_size']\n",
    "        self._embedding_layer_size = model_parameters['embedding_layer_size']\n",
    "        self._num_layers           = model_parameters['num_layers']\n",
    "        self._cell_type            = model_parameters['cell_type'].lower()\n",
    "        self._dropout              = model_parameters['dropout']\n",
    "        self._layer_normalization  = model_parameters['layer_normalization']\n",
    "        \n",
    "        super(SmilesLSTM, self).__init__()\n",
    "        # define model\n",
    "        self._embedding = nn.Embedding(num_embeddings=len(self.vocabulary),\n",
    "                               embedding_dim=self._embedding_layer_size)\n",
    "        \n",
    "        self._reccurent = nn.LSTM(input_size=len(self.vocabulary),\n",
    "                          hidden_size=self._layer_size,\n",
    "                          num_layers=self._num_layers,\n",
    "                          dropout=self._dropout,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self._linear = nn.Linear(self._layer_size, len(self.vocabulary))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        embedded_data = self._embedding(x) # (batch,seq,embedding)\n",
    "\n",
    "        output_vector, hidden_state_out, cell_state = self._reccurent(embedded_data)\n",
    "\n",
    "        output_vector = output_vector.reshape(-1, self._layer_size)\n",
    "\n",
    "        output_data = self._linear(output_vector).view(batch_size, seq_size, -1)\n",
    "\n",
    "        return output_data\n",
    "    \n",
    "    def get_params(self) -> dict:\n",
    "        \"\"\"\n",
    "        Returns the configuration parameters of the model.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"dropout\"              : self._dropout,\n",
    "            \"layer_size\"           : self._layer_size,\n",
    "            \"num_layers\"           : self._num_layers,\n",
    "            \"cell_type\"            : self._cell_type,\n",
    "            \"embedding_layer_size\" : self._embedding_layer_size\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout': 0.2,\n",
       " 'layer_size': 5,\n",
       " 'num_layers': 3,\n",
       " 'cell_type': 'lstm',\n",
       " 'embedding_layer_size': 5}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network parameters\n",
    "model_parameters = {\n",
    "    'num_layers'          : 3,\n",
    "    'layer_size'          : 5,\n",
    "    'cell_type'           : 'lstm',\n",
    "    'embedding_layer_size': 5,\n",
    "    'dropout'             : 0.2,\n",
    "    'layer_normalization' : True,\n",
    "}\n",
    "\n",
    "model = SmilesLSTM(vocabulary, tk, model_parameters)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into train and combined val/test sets\n",
    "train_data, val_test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# Splitting the combined val/test set into separate val and test sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train     = train_data.copy()\n",
    "test      = test_data.copy()\n",
    "valid     = val_data.copy()\n",
    "\n",
    "# create a vocabulary using all SMILES in df\n",
    "dataset = df['source'].unique().tolist() + df['target'].unique().tolist()\n",
    "dataset = np.unique(dataset).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_sequence(tokenizer_array, desired_length):\n",
    "    padded_sequence = pad_sequences([tokenizer_array], maxlen=desired_length, padding='post')[0]\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = train['source'].tolist()\n",
    "y = train['target'].tolist()\n",
    "\n",
    "def preprocess_smiles_data(X):\n",
    "    for row in range(len(X)):\n",
    "        X[row] = tk.tokenize(X[row])\n",
    "        X[row] = vocabulary.encode(X[row] )\n",
    "        X[row]  = pad_sequence(X[row], 200 )\n",
    "        X[row]  = torch.tensor([X[row] ])\n",
    "    return X\n",
    "\n",
    "X = preprocess_smiles_data(X)\n",
    "y = preprocess_smiles_data(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(X_[0].shape)\n",
    "print(numerical_indices_smi_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 86, got 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 4\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(numerical_indices_smi_sample)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[166], line 35\u001b[0m, in \u001b[0;36mSmilesLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     33\u001b[0m     embedded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding(x) \u001b[38;5;66;03m# (batch,seq,embedding)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     output_vector, hidden_state_out, cell_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reccurent(embedded_data)\n\u001b[0;32m     37\u001b[0m     output_vector \u001b[38;5;241m=\u001b[39m output_vector\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_size)\n\u001b[0;32m     39\u001b[0m     output_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linear(output_vector)\u001b[38;5;241m.\u001b[39mview(batch_size, seq_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    729\u001b[0m                        ):\n\u001b[1;32m--> 730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 86, got 5"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    \n",
    "    model.zero_grad()\n",
    "    out = model(numerical_indices_smi_sample)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model: SmilesLSTM, input_smiles : Union[dict, str],\n",
    "                 epochs : int=10, learning_rate : float=0.0001,\n",
    "                 batch_size : int=250, shuffle : bool=True,\n",
    "                 augment : int=0, output_model_path : str=\"./output/\", start_epoch : int=0,\n",
    "                 learning_rate_scheduler : str=\"StepLR\", gamma : float=0.8,\n",
    "                 eval_num_samples : int=64, eval_batch_size : int=64) -> None:\n",
    "        \n",
    "        # define the model\n",
    "        self._model = model\n",
    "\n",
    "        # define parameters\n",
    "        self._batch_size        = batch_size\n",
    "        self._learning_rate     = learning_rate\n",
    "        self._epochs            = epochs\n",
    "        self._start_epoch       = start_epoch\n",
    "        self._output_model_path = output_model_path\n",
    "        self._shuffle           = shuffle\n",
    "        self._use_augmentation  = augment\n",
    "        self._eval_num_samples  = eval_num_samples\n",
    "        self._eval_batch_size   = eval_batch_size\n",
    "\n",
    "        # define the data\n",
    "        (self._train_dataloader,\n",
    "         self._test_dataloader,\n",
    "         self._valid_dataloader) = self._load_smiles(input_smiles=input_smiles)\n",
    "        \n",
    "        # define the optimizer and scheduler\n",
    "        self._optimizer = torch.optim.Adam(params=self._model.network.parameters(),\n",
    "                                           lr=self._learning_rate)\n",
    "        \n",
    "        # placeholders for the loss\n",
    "        self._train_loss      = None\n",
    "        self._valid_loss      = None\n",
    "        self._best_valid_loss = None\n",
    "        self._best_epoch      = None\n",
    "        \n",
    "        def _initialize_dataloader(self, smiles_list : list) -> toch.utils.data.Dataloader:\n",
    "            dataset = Dataset(smiles_list=smiles_list,\n",
    "                              vocabulary=self._model.vocabulary,\n",
    "                              tokenizer=SMILESTokenizer())\n",
    "            \n",
    "            dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                                     batch_size=self._batch_size,\n",
    "                                                     shuffle=self._shuffle,\n",
    "                                                     collate_fn=Dataset.collate_fn,\n",
    "                                                     drop_last=True)\n",
    "            return dataloader\n",
    "        \n",
    "        def _load_smiles(self, input_smiles : Union[dict, str]) -> Tuple[list, list, list]:\n",
    "            # get values from dictionary\n",
    "            train_smiles = input_smiles[\"train\"]\n",
    "            test_smiles  = input_smiles[\"test\"]\n",
    "            valid_smiles = input_smiles[\"valid\"]\n",
    "\n",
    "            # create the dataloader from the SMILES lists\n",
    "            train_dataloader = self._initialize_dataloader(smiles_list=train_smiles)\n",
    "            test_dataloader  = self._initialize_dataloader(smiles_list=test_smiles)\n",
    "            valid_dataloader = self._initialize_dataloader(smiles_list=valid_smiles)\n",
    "            \n",
    "            return train_dataloader, test_dataloader, valid_dataloader      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validation / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Splitting the data into train and combined val/test sets\n",
    "train_data, val_test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the combined val/test set into separate val and test sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Printing the sizes of the resulting splits\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(val_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train     = train_data.copy()\n",
    "test      = test_data.copy()\n",
    "valid     = val_data.copy()\n",
    "\n",
    "# create a vocabulary using all SMILES in df\n",
    "dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "dataset = np.unique(dataset).tolist()\n",
    "\n",
    "tokenizer = SMILESTokenizer()\n",
    "vocab     = create_vocabulary(smiles_list=dataset,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    canonical=False)\n",
    "\n",
    "MAX_LENGTH = max(len(v) for v in dataset)\n",
    "\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'Max length: {MAX_LENGTH}.\\n')\n",
    "# print(f'The unique tokens are: \\n{vocabulary.tokens()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = SMILESTokenizer()\n",
    "vocab = Vocabulary()\n",
    "\n",
    "smi_sample = 'CCBr'\n",
    "tokenized_smi_sample = tk.tokenize(smi_sample, with_begin_and_end=False)\n",
    "print(tokenized_smi_sample)\n",
    "vocabulary.encode(tokenized_smi_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for pad sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_sequence(tokenizer_array, desired_length):\n",
    "    padded_sequence = pad_sequences([tokenizer_array], maxlen=desired_length, padding='post')[0]\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [train, test, valid]:\n",
    "    for c in d.columns:\n",
    "        d[c] = d[c].apply(lambda x: tk.tokenize(x, with_begin_and_end=False))\n",
    "        d[c] = d[c].apply(lambda x: vocabulary.encode(x).astype(int))\n",
    "        d[c] = d[c].apply(lambda x: pad_sequence(x, MAX_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the source and target columns into numpy arrays\n",
    "trainX = np.array(train['source'].tolist())\n",
    "trainY = np.array(train['target'].tolist())\n",
    "\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (trainX.shape[1], 1)  # Assuming you want to feed one feature at a time\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=NUM_EMBEDDING, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH))\n",
    "model.add(LSTM(units=128, input_shape=input_shape))\n",
    "model.add(Dense(units=trainY.shape[1], activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, trainY, epochs=3, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the source and target columns into numpy arrays\n",
    "testX = np.array(test['source'].tolist())\n",
    "testY = np.array(test['target'].tolist())\n",
    "\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(testX)\n",
    "pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(testY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
