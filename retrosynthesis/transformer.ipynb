{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "# ignore some deprecation warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load main data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...</td>\n",
       "      <td>CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1</td>\n",
       "      <td>COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C</td>\n",
       "      <td>CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O</td>\n",
       "      <td>CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fc1cc2c(NC3CCCCCC3)ncnc2cn1</td>\n",
       "      <td>Fc1cc2c(Cl)ncnc2cn1.NC1CCCCCC1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...   \n",
       "1      Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1    \n",
       "2    CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C    \n",
       "3            CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O    \n",
       "4                       Fc1cc2c(NC3CCCCCC3)ncnc2cn1    \n",
       "\n",
       "                                              target  \n",
       "0   CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...  \n",
       "1      COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1  \n",
       "2   CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...  \n",
       "3    CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1  \n",
       "4                     Fc1cc2c(Cl)ncnc2cn1.NC1CCCCCC1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('retrosynthesis-all', header=None)\n",
    "df['source'] = df[0].apply(lambda x: x.split('>>')[0])\n",
    "df['target'] = df[0].apply(lambda x: x.split('>>')[1])\n",
    "df.drop(0, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Vocabulary: How is it generated?\n",
    "\n",
    "The model's Vocabulary handles the transformation of SMILES strings into a sequence of tokens. Tokens are the pre-defined lowest and indivisible unit of string text. In Natural Language Processing (NLP), tokens are typically defined on the word or character level. The level of tokenization dictates *what* the model can output, e.g., if tokenization on the character level is used, then the model outputs individual characters.\n",
    "\n",
    "For generative SMILES models, tokenization is performed on the character level where each token *loosely* maps to a unique atom type (brackets, \"(\" for example indicate branching and thus, do not map to an atom but rather gives connectivity information).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, Vocabulary, create_vocabulary\n",
    "\n",
    "tk = SMILESTokenizer()\n",
    "vocab = Vocabulary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'Br']\n"
     ]
    }
   ],
   "source": [
    "# smi_sample = df['source'].iloc[123]\n",
    "smi_sample = 'CCBr'\n",
    "print(tk.tokenize(smi_sample, with_begin_and_end=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in a natural language like english where one's vocabulary controls what sentences can be formulated, a molecular generative model's vocabulary controls what kind of atoms can be proposed - sentences in this context are molecules\n",
    "\n",
    "In order for the token representations of SMILES sequences to be passed into a machine learning model, they must be transformed into a numerical representation. This is done in the Vocabulary class where each unique token is mapped to a numerical index. This is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 unique tokens in the vocabulary.\n",
      "\n",
      "The unique tokens are: \n",
      "['$', '^', ' ', '#', '(', ')', '-', '.', '/', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH3-]', '[Br-]', '[C-]', '[C@@H]', '[C@@]', '[C@H]', '[C@]', '[Cl+3]', '[Cl-]', '[Cu]', '[Fe]', '[I+]', '[K]', '[Li]', '[Mg+]', '[Mg]', '[N+]', '[N-]', '[N@+]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[NH4+]', '[O-]', '[OH-]', '[P+]', '[PH2]', '[PH4]', '[PH]', '[Pd]', '[Pt]', '[S+]', '[S-]', '[S@@]', '[S@]', '[SH]', '[Se]', '[SiH2]', '[SiH]', '[Si]', '[SnH]', '[Sn]', '[Zn+]', '[Zn]', '[n+]', '[n-]', '[nH]', '[s+]', '[se]', '\\\\', 'c', 'n', 'o', 's']\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary using all SMILES in df\n",
    "smiles_dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "smiles_dataset = np.unique(smiles_dataset).tolist()\n",
    "\n",
    "vocabulary = create_vocabulary(smiles_list=smiles_dataset, tokenizer=tk)\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'The unique tokens are: \\n{vocabulary.tokens()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCBr\n",
      "['C', 'C', 'Br']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21., 21., 20.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(smi_sample)\n",
    "tokenized_smi_sample = tk.tokenize(smi_sample, with_begin_and_end=False)\n",
    "print(tokenized_smi_sample)\n",
    "\n",
    "vocabulary.encode(tokenized_smi_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes *how* the numerical representation of tokens are transformed into an input vector known as the embedding that will act as the input to the RNN.\n",
    "\n",
    "An Embedding Layer is essentially a look-up table. In the constructor above, `num_embeddings` refers to the Vocabulary size. \n",
    "\n",
    "`num_embeddings` denotes how many vectors to initialize. \n",
    "\n",
    "Since we have n unique tokens, we need _ different vectors: 1 for each unique token. This is why `num_embeddings` is _ in this example. `embedding_dim` denotes the dimension of the embedding vector. 5 is arbitrarily chosen here just for easy visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an \"Embedding layer\"\n",
    "EMBEDDING_DIM = 5\n",
    "NUM_EMBEDDING = len(vocabulary)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=NUM_EMBEDDING,\n",
    "                               embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# only 1 layer of LSTM cells is initialized here for the sake of illustration\n",
    "# input_size = 5 because we previously defined the \"embedding_dim\" of the Embedding layer to be 5\n",
    "# hidden_size = 5 is arbitrarily chosen for easy visualization\n",
    "recurrent_layer = nn.LSTM(input_size=EMBEDDING_DIM,\n",
    "                          hidden_size=5,\n",
    "                          num_layers=1,\n",
    "                          dropout=0,\n",
    "                          batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical indices of bromoethane:\n",
      " tensor([[21, 21, 20]])\n",
      "\n",
      "Embedding:\n",
      " tensor([[[-1.4637, -0.8914,  0.2654, -0.5349, -0.8725],\n",
      "         [-1.4637, -0.8914,  0.2654, -0.5349, -0.8725],\n",
      "         [-0.8563, -0.6271,  0.2001, -0.6347, -1.1391]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# get the numerical indices of bromoethane\n",
    "numerical_indices_smi_sample = torch.LongTensor([vocabulary.encode(tokenized_smi_sample).astype(int)])\n",
    "print(f\"Numerical indices of bromoethane:\\n {numerical_indices_smi_sample}\\n\")\n",
    "\n",
    "embedding = embedding_layer(numerical_indices_smi_sample)\n",
    "print(f\"Embedding:\\n {embedding}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validation / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45033, 2)\n",
      "Train data size: 36026\n",
      "Validation data size: 7205\n",
      "Test data size: 1802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Splitting the data into train and combined val/test sets\n",
    "train_data, val_test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the combined val/test set into separate val and test sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Printing the sizes of the resulting splits\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(val_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('src/')\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from smiles_lstm.model.smiles_lstm import SmilesLSTM\n",
    "from smiles_lstm.model.smiles_trainer import SmilesTrainer\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, create_vocabulary\n",
    "from smiles_lstm.utils import load\n",
    "from smiles_lstm.utils.misc import suppress_warnings\n",
    "\n",
    "zinc_path = Path(\"./scripts/data/zinc/\")\n",
    "train     = load.smiles(path=(zinc_path.joinpath(\"train.smi\")))\n",
    "test      = load.smiles(path=(zinc_path.joinpath(\"test.smi\")))\n",
    "valid     = load.smiles(path=(zinc_path.joinpath(\"valid.smi\")))\n",
    "\n",
    "dataset = train + test + valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train     = train_data\n",
    "test      = test_data\n",
    "valid     = val_data\n",
    "\n",
    "# create a vocabulary using all SMILES in df\n",
    "dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "dataset = np.unique(dataset).tolist()\n",
    "\n",
    "tokenizer = SMILESTokenizer()\n",
    "vocab     = create_vocabulary(smiles_list=dataset,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    canonical=False)\n",
    "\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'The unique tokens are: \\n{vocabulary.tokens()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer object expects data in a dictionary, so reorganizing it so\n",
    "SMILES_dict = {\"train\" : train, \"valid\" : valid, \"test\"  : test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
