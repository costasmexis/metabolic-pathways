{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "# ignore some deprecation warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load main data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...</td>\n",
       "      <td>CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1</td>\n",
       "      <td>COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C</td>\n",
       "      <td>CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O</td>\n",
       "      <td>CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fc1cc2c(NC3CCCCCC3)ncnc2cn1</td>\n",
       "      <td>Fc1cc2c(Cl)ncnc2cn1.NC1CCCCCC1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...   \n",
       "1      Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1    \n",
       "2    CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C    \n",
       "3            CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O    \n",
       "4                       Fc1cc2c(NC3CCCCCC3)ncnc2cn1    \n",
       "\n",
       "                                              target  \n",
       "0   CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...  \n",
       "1      COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1  \n",
       "2   CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...  \n",
       "3    CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1  \n",
       "4                     Fc1cc2c(Cl)ncnc2cn1.NC1CCCCCC1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('retrosynthesis-all', header=None)\n",
    "df['source'] = df[0].apply(lambda x: x.split('>>')[0])\n",
    "df['target'] = df[0].apply(lambda x: x.split('>>')[1])\n",
    "df.drop(0, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Vocabulary: How is it generated?\n",
    "\n",
    "The model's Vocabulary handles the transformation of SMILES strings into a sequence of tokens. Tokens are the pre-defined lowest and indivisible unit of string text. In Natural Language Processing (NLP), tokens are typically defined on the word or character level. The level of tokenization dictates *what* the model can output, e.g., if tokenization on the character level is used, then the model outputs individual characters.\n",
    "\n",
    "For generative SMILES models, tokenization is performed on the character level where each token *loosely* maps to a unique atom type (brackets, \"(\" for example indicate branching and thus, do not map to an atom but rather gives connectivity information).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, Vocabulary, create_vocabulary\n",
    "\n",
    "tk = SMILESTokenizer()\n",
    "vocab = Vocabulary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'Br']\n"
     ]
    }
   ],
   "source": [
    "# smi_sample = df['source'].iloc[123]\n",
    "smi_sample = 'CCBr'\n",
    "print(tk.tokenize(smi_sample, with_begin_and_end=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in a natural language like english where one's vocabulary controls what sentences can be formulated, a molecular generative model's vocabulary controls what kind of atoms can be proposed - sentences in this context are molecules\n",
    "\n",
    "In order for the token representations of SMILES sequences to be passed into a machine learning model, they must be transformed into a numerical representation. This is done in the Vocabulary class where each unique token is mapped to a numerical index. This is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 unique tokens in the vocabulary.\n",
      "\n",
      "The unique tokens are: \n",
      "['$', '^', ' ', '#', '(', ')', '-', '.', '/', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH3-]', '[Br-]', '[C-]', '[C@@H]', '[C@@]', '[C@H]', '[C@]', '[Cl+3]', '[Cl-]', '[Cu]', '[Fe]', '[I+]', '[K]', '[Li]', '[Mg+]', '[Mg]', '[N+]', '[N-]', '[N@+]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[NH4+]', '[O-]', '[OH-]', '[P+]', '[PH2]', '[PH4]', '[PH]', '[Pd]', '[Pt]', '[S+]', '[S-]', '[S@@]', '[S@]', '[SH]', '[Se]', '[SiH2]', '[SiH]', '[Si]', '[SnH]', '[Sn]', '[Zn+]', '[Zn]', '[n+]', '[n-]', '[nH]', '[s+]', '[se]', '\\\\', 'c', 'n', 'o', 's']\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary using all SMILES in df\n",
    "smiles_dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "smiles_dataset = np.unique(smiles_dataset).tolist()\n",
    "\n",
    "vocabulary = create_vocabulary(smiles_list=smiles_dataset, tokenizer=tk)\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'The unique tokens are: \\n{vocabulary.tokens()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCBr\n",
      "['C', 'C', 'Br']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21., 21., 20.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(smi_sample)\n",
    "tokenized_smi_sample = tk.tokenize(smi_sample, with_begin_and_end=False)\n",
    "print(tokenized_smi_sample)\n",
    "\n",
    "vocabulary.encode(tokenized_smi_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Î¤he `encode` method in the `Vocabulary` class takes a list of tokens and returns its numerical indices the indices are what we expect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes *how* the numerical representation of tokens are transformed into an input vector known as the embedding that will act as the input to the RNN.\n",
    "\n",
    "An Embedding Layer is essentially a look-up table. In the constructor above, `num_embeddings` refers to the Vocabulary size. \n",
    "\n",
    "`num_embeddings` denotes how many vectors to initialize. \n",
    "\n",
    "Since we have n unique tokens, we need _ different vectors: 1 for each unique token. This is why `num_embeddings` is _ in this example. `embedding_dim` denotes the dimension of the embedding vector. 5 is arbitrarily chosen here just for easy visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an \"Embedding layer\"\n",
    "EMBEDDING_DIM = 5\n",
    "NUM_EMBEDDING = len(vocabulary)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=NUM_EMBEDDING,\n",
    "                               embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# only 1 layer of LSTM cells is initialized here for the sake of illustration\n",
    "# input_size = 5 because we previously defined the \"embedding_dim\" of the Embedding layer to be 5\n",
    "# hidden_size = 5 is arbitrarily chosen for easy visualization\n",
    "recurrent_layer = nn.LSTM(input_size=EMBEDDING_DIM,\n",
    "                          hidden_size=5,\n",
    "                          num_layers=1,\n",
    "                          dropout=0,\n",
    "                          batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical indices of bromoethane:\n",
      " tensor([[21, 21, 20]])\n",
      "\n",
      "Embedding:\n",
      " tensor([[[ 0.1534, -0.3632, -0.1740,  1.8087, -0.1279],\n",
      "         [ 0.1534, -0.3632, -0.1740,  1.8087, -0.1279],\n",
      "         [ 1.6083, -0.8310, -1.3849, -0.7949,  0.2275]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# get the numerical indices of bromoethane\n",
    "numerical_indices_smi_sample = torch.LongTensor([vocabulary.encode(tokenized_smi_sample).astype(int)])\n",
    "print(f\"Numerical indices of bromoethane:\\n {numerical_indices_smi_sample}\\n\")\n",
    "\n",
    "embedding = embedding_layer(numerical_indices_smi_sample)\n",
    "print(f\"Embedding:\\n {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5])\n",
      "torch.Size([1, 3, 5])\n",
      "torch.Size([1, 3, 20])\n",
      "tensor([[[0.0742, 0.0401, 0.0410, 0.0523, 0.0651, 0.0588, 0.0472, 0.0356,\n",
      "          0.0788, 0.0493, 0.0742, 0.0334, 0.0660, 0.0341, 0.0478, 0.0344,\n",
      "          0.0509, 0.0434, 0.0397, 0.0338],\n",
      "         [0.0770, 0.0397, 0.0425, 0.0506, 0.0670, 0.0565, 0.0466, 0.0366,\n",
      "          0.0791, 0.0493, 0.0738, 0.0338, 0.0663, 0.0342, 0.0464, 0.0338,\n",
      "          0.0491, 0.0440, 0.0403, 0.0332],\n",
      "         [0.0665, 0.0439, 0.0417, 0.0549, 0.0586, 0.0675, 0.0510, 0.0323,\n",
      "          0.0727, 0.0486, 0.0706, 0.0344, 0.0655, 0.0338, 0.0527, 0.0350,\n",
      "          0.0509, 0.0455, 0.0398, 0.0340]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-2.6014, -3.2175, -3.1939, -2.9500, -2.7316, -2.8341, -3.0533,\n",
      "          -3.3361, -2.5405, -3.0096, -2.6011, -3.3979, -2.7184, -3.3784,\n",
      "          -3.0409, -3.3710, -2.9770, -3.1381, -3.2272, -3.3879],\n",
      "         [-2.5640, -3.2260, -3.1572, -2.9831, -2.7025, -2.8742, -3.0666,\n",
      "          -3.3071, -2.5376, -3.0095, -2.6061, -3.3864, -2.7130, -3.3760,\n",
      "          -3.0703, -3.3869, -3.0146, -3.1224, -3.2110, -3.4050],\n",
      "         [-2.7108, -3.1250, -3.1766, -2.9030, -2.8365, -2.6955, -2.9753,\n",
      "          -3.4330, -2.6210, -3.0249, -2.6506, -3.3693, -2.7254, -3.3879,\n",
      "          -2.9434, -3.3512, -2.9784, -3.0900, -3.2233, -3.3816]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "Original SMILES string: CCBr\n",
      "\n",
      "The unique tokens are: \n",
      "['$', '^', ' ', '#', '(', ')', '-', '.', '/', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH3-]', '[Br-]', '[C-]', '[C@@H]', '[C@@]', '[C@H]', '[C@]', '[Cl+3]', '[Cl-]', '[Cu]', '[Fe]', '[I+]', '[K]', '[Li]', '[Mg+]', '[Mg]', '[N+]', '[N-]', '[N@+]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[NH4+]', '[O-]', '[OH-]', '[P+]', '[PH2]', '[PH4]', '[PH]', '[Pd]', '[Pt]', '[S+]', '[S-]', '[S@@]', '[S@]', '[SH]', '[Se]', '[SiH2]', '[SiH]', '[Si]', '[SnH]', '[Sn]', '[Zn+]', '[Zn]', '[n+]', '[n-]', '[nH]', '[s+]', '[se]', '\\\\', 'c', 'n', 'o', 's']\n",
      "\n",
      "At time step 1, the generative model proposes / as the most probable token and the correct token is C\n",
      "At time step 2, the generative model proposes / as the most probable token and the correct token is C\n",
      "At time step 3, the generative model proposes / as the most probable token and the correct token is B\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)\n",
    "\n",
    "# let's run the embedding through the recurrent layer\n",
    "rnn_output, (hidden_state, cell_state) = recurrent_layer(embedding)\n",
    "\n",
    "print(rnn_output.shape)\n",
    "\n",
    "\n",
    "\n",
    "# initialize the linear layer\n",
    "# in_features = 5 as that is the hidden_size defined in the recurrent layer above\n",
    "# out_features = 20 as that is the size of the Vocabulary\n",
    "linear_layer = nn.Linear(in_features=5,\n",
    "                         out_features=20)\n",
    "\n",
    "linear_output = linear_layer(rnn_output)\n",
    "\n",
    "# verify the shape of the linear output is what we expect:\n",
    "# (batch size) x (sequence length) x (vocabulary size)\n",
    "print(linear_output.shape)\n",
    "\n",
    "# let's first show the normal softmax output\n",
    "# recall the output from the linear layer has dimensions: (batch size) x (sequence length) x (vocabulary size)\n",
    "# therefore, dim=2 because we want to compute the softmax over the vocabulary to obtain a probability for each token\n",
    "softmax = linear_output.softmax(dim=2)\n",
    "print(softmax)\n",
    "\n",
    "# let's now show the log-softmax output\n",
    "log_softmax = linear_output.log_softmax(dim=2)\n",
    "print(log_softmax)\n",
    "\n",
    "# log-softmax to token probabilities\n",
    "# recall our original SMILES \n",
    "print(f\"Original SMILES string: {smi_sample}\\n\")\n",
    "\n",
    "# recall our vocabulary\n",
    "print(f\"The unique tokens are: \\n{vocabulary.tokens()}\\n\")\n",
    "\n",
    "# we now extract the max value in each tensor of the log-softmax output above and the corresponding token\n",
    "most_probable_tokens = log_softmax.argmax(dim=2).flatten().tolist()\n",
    "for idx, (correct_token, most_probable_token) in enumerate(zip(smi_sample, most_probable_tokens)):\n",
    "    print(f\"At time step {idx+1}, the generative model proposes {vocabulary.tokens()[most_probable_token]} as the most probable token and the correct token is {correct_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validation / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45033, 2)\n",
      "Train data size: 36026\n",
      "Validation data size: 7205\n",
      "Test data size: 1802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Splitting the data into train and combined val/test sets\n",
    "train_data, val_test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the combined val/test set into separate val and test sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Printing the sizes of the resulting splits\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(val_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the NMT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('src/')\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from smiles_lstm.model.smiles_lstm import SmilesLSTM\n",
    "from smiles_lstm.model.smiles_trainer import SmilesTrainer\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, create_vocabulary\n",
    "from smiles_lstm.utils import load\n",
    "from smiles_lstm.utils.misc import suppress_warnings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 unique tokens in the vocabulary.\n",
      "\n",
      "Max length: 198.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train     = train_data.copy()\n",
    "test      = test_data.copy()\n",
    "valid     = val_data.copy()\n",
    "\n",
    "# create a vocabulary using all SMILES in df\n",
    "dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "dataset = np.unique(dataset).tolist()\n",
    "\n",
    "tokenizer = SMILESTokenizer()\n",
    "vocab     = create_vocabulary(smiles_list=dataset,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    canonical=False)\n",
    "\n",
    "MAX_LENGTH = max(len(v) for v in dataset)\n",
    "\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'Max length: {MAX_LENGTH}.\\n')\n",
    "# print(f'The unique tokens are: \\n{vocabulary.tokens()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'Br']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21., 21., 20.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = SMILESTokenizer()\n",
    "vocab = Vocabulary()\n",
    "\n",
    "smi_sample = 'CCBr'\n",
    "tokenized_smi_sample = tk.tokenize(smi_sample, with_begin_and_end=False)\n",
    "print(tokenized_smi_sample)\n",
    "vocabulary.encode(tokenized_smi_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for pad sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_sequence(tokenizer_array, desired_length):\n",
    "    padded_sequence = pad_sequences([tokenizer_array], maxlen=desired_length, padding='post')[0]\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [train, test, valid]:\n",
    "    for c in d.columns:\n",
    "        d[c] = d[c].apply(lambda x: tk.tokenize(x, with_begin_and_end=False))\n",
    "        d[c] = d[c].apply(lambda x: vocabulary.encode(x).astype(int))\n",
    "        d[c] = d[c].apply(lambda x: pad_sequence(x, MAX_LENGTH))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36026, 198) (36026, 198)\n"
     ]
    }
   ],
   "source": [
    "# Convert the source and target columns into numpy arrays\n",
    "trainX = np.array(train['source'].tolist())\n",
    "trainY = np.array(train['target'].tolist())\n",
    "\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "18/18 [==============================] - 3s 26ms/step - loss: -3.1878 - accuracy: 0.0028\n",
      "Epoch 2/3\n",
      "18/18 [==============================] - 0s 27ms/step - loss: -23.7747 - accuracy: 0.0012\n",
      "Epoch 3/3\n",
      "18/18 [==============================] - 1s 28ms/step - loss: -46.9983 - accuracy: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd6f406980>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (trainX.shape[1], 1)  # Assuming you want to feed one feature at a time\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=NUM_EMBEDDING, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH))\n",
    "model.add(LSTM(units=128, input_shape=input_shape))\n",
    "model.add(Dense(units=trainY.shape[1], activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, trainY, epochs=3, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1802, 198) (1802, 198)\n"
     ]
    }
   ],
   "source": [
    "# Convert the source and target columns into numpy arrays\n",
    "testX = np.array(test['source'].tolist())\n",
    "testY = np.array(test['target'].tolist())\n",
    "\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1802 rows Ã 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  188  189  190  \\\n",
       "0      21   21    4   21    5   25    4   21    4   18  ...    0    0    0   \n",
       "1      26   18   21    9   25   21    4   18   26    5  ...    0    0    0   \n",
       "2      21   82    9   82    4   21   21   25   10   21  ...    0    0    0   \n",
       "3      21   25    9   21   21   21    4   25   10   21  ...    0    0    0   \n",
       "4      21   21    4   21    5    4   21    5   26   21  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1797   26   18   21    4   25   82    9   82   82   83  ...    0    0    0   \n",
       "1798   21   21    4   21    5    4   21    5   26   21  ...    0    0    0   \n",
       "1799   21   21    4   18   26    5   21    4   26    5  ...    0    0    0   \n",
       "1800   21   21   21   21    4   18   26    5   25   82  ...    0    0    0   \n",
       "1801   23   21    4   23    5    4   23    5   82    9  ...    0    0    0   \n",
       "\n",
       "      191  192  193  194  195  196  197  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1797    0    0    0    0    0    0    0  \n",
       "1798    0    0    0    0    0    0    0  \n",
       "1799    0    0    0    0    0    0    0  \n",
       "1800    0    0    0    0    0    0    0  \n",
       "1801    0    0    0    0    0    0    0  \n",
       "\n",
       "[1802 rows x 198 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(testX)\n",
    "pd.DataFrame(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>78</td>\n",
       "      <td>82</td>\n",
       "      <td>10</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1802 rows Ã 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  188  189  190  \\\n",
       "0       2   21   21    4   21    5   25    4   21    4  ...    0    0    0   \n",
       "1       2   25   21   36    4   26    5   82    9   82  ...    0    0    0   \n",
       "2       2   21   82    9   78   82   10   82   82   82  ...    0    0    0   \n",
       "3       2   21   25    9   21   21   21    4   25   10  ...    0    0    0   \n",
       "4       2   21   21    4   21    5    4   21    5   26  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1797    2   25   82    9   82   82   83   82   82    9  ...    0    0    0   \n",
       "1798    2   21   21    4   21    5    4   21    5   26  ...    0    0    0   \n",
       "1799    2   21   21   26   21    4   26   21   21    5  ...    0    0    0   \n",
       "1800    2   21   21   21   21    4   18   26    5   22  ...    0    0    0   \n",
       "1801    2   25   82    9   82   82   82   82    4    6  ...    0    0    0   \n",
       "\n",
       "      191  192  193  194  195  196  197  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1797    0    0    0    0    0    0    0  \n",
       "1798    0    0    0    0    0    0    0  \n",
       "1799    0    0    0    0    0    0    0  \n",
       "1800    0    0    0    0    0    0    0  \n",
       "1801    0    0    0    0    0    0    0  \n",
       "\n",
       "[1802 rows x 198 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(testY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
