{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "# ignore some deprecation warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load main data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...</td>\n",
       "      <td>CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1</td>\n",
       "      <td>COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C</td>\n",
       "      <td>CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O</td>\n",
       "      <td>CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fc1cc2c(NC3CCCCCC3)ncnc2cn1</td>\n",
       "      <td>Fc1cc2c(Cl)ncnc2cn1.NC1CCCCCC1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...   \n",
       "1      Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1    \n",
       "2    CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C    \n",
       "3            CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O    \n",
       "4                       Fc1cc2c(NC3CCCCCC3)ncnc2cn1    \n",
       "\n",
       "                                              target  \n",
       "0   CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...  \n",
       "1      COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1  \n",
       "2   CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...  \n",
       "3    CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1  \n",
       "4                     Fc1cc2c(Cl)ncnc2cn1.NC1CCCCCC1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('retrosynthesis-all', header=None)\n",
    "df['source'] = df[0].apply(lambda x: x.split('>>')[0])\n",
    "df['target'] = df[0].apply(lambda x: x.split('>>')[1])\n",
    "df.drop(0, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Vocabulary: How is it generated?\n",
    "\n",
    "The model's Vocabulary handles the transformation of SMILES strings into a sequence of tokens. Tokens are the pre-defined lowest and indivisible unit of string text. In Natural Language Processing (NLP), tokens are typically defined on the word or character level. The level of tokenization dictates *what* the model can output, e.g., if tokenization on the character level is used, then the model outputs individual characters.\n",
    "\n",
    "For generative SMILES models, tokenization is performed on the character level where each token *loosely* maps to a unique atom type (brackets, \"(\" for example indicate branching and thus, do not map to an atom but rather gives connectivity information).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, Vocabulary, create_vocabulary\n",
    "\n",
    "tk = SMILESTokenizer()\n",
    "vocab = Vocabulary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'Br']\n"
     ]
    }
   ],
   "source": [
    "# smi_sample = df['source'].iloc[123]\n",
    "smi_sample = 'CCBr'\n",
    "print(tk.tokenize(smi_sample, with_begin_and_end=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in a natural language like english where one's vocabulary controls what sentences can be formulated, a molecular generative model's vocabulary controls what kind of atoms can be proposed - sentences in this context are molecules\n",
    "\n",
    "In order for the token representations of SMILES sequences to be passed into a machine learning model, they must be transformed into a numerical representation. This is done in the Vocabulary class where each unique token is mapped to a numerical index. This is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 unique tokens in the vocabulary.\n",
      "\n",
      "The unique tokens are: \n",
      "['$', '^', ' ', '#', '(', ')', '-', '.', '/', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH3-]', '[Br-]', '[C-]', '[C@@H]', '[C@@]', '[C@H]', '[C@]', '[Cl+3]', '[Cl-]', '[Cu]', '[Fe]', '[I+]', '[K]', '[Li]', '[Mg+]', '[Mg]', '[N+]', '[N-]', '[N@+]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[NH4+]', '[O-]', '[OH-]', '[P+]', '[PH2]', '[PH4]', '[PH]', '[Pd]', '[Pt]', '[S+]', '[S-]', '[S@@]', '[S@]', '[SH]', '[Se]', '[SiH2]', '[SiH]', '[Si]', '[SnH]', '[Sn]', '[Zn+]', '[Zn]', '[n+]', '[n-]', '[nH]', '[s+]', '[se]', '\\\\', 'c', 'n', 'o', 's']\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary using all SMILES in df\n",
    "smiles_dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "smiles_dataset = np.unique(smiles_dataset).tolist()\n",
    "\n",
    "vocabulary = create_vocabulary(smiles_list=smiles_dataset, tokenizer=tk)\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'The unique tokens are: \\n{vocabulary.tokens()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCBr\n",
      "['C', 'C', 'Br']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21., 21., 20.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(smi_sample)\n",
    "tokenized_smi_sample = tk.tokenize(smi_sample, with_begin_and_end=False)\n",
    "print(tokenized_smi_sample)\n",
    "\n",
    "vocabulary.encode(tokenized_smi_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes *how* the numerical representation of tokens are transformed into an input vector known as the embedding that will act as the input to the RNN.\n",
    "\n",
    "An Embedding Layer is essentially a look-up table. In the constructor above, `num_embeddings` refers to the Vocabulary size. \n",
    "\n",
    "`num_embeddings` denotes how many vectors to initialize. \n",
    "\n",
    "Since we have n unique tokens, we need _ different vectors: 1 for each unique token. This is why `num_embeddings` is _ in this example. `embedding_dim` denotes the dimension of the embedding vector. 5 is arbitrarily chosen here just for easy visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an \"Embedding layer\"\n",
    "EMBEDDING_DIM = 5\n",
    "NUM_EMBEDDING = len(vocabulary)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=NUM_EMBEDDING,\n",
    "                               embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# only 1 layer of LSTM cells is initialized here for the sake of illustration\n",
    "# input_size = 5 because we previously defined the \"embedding_dim\" of the Embedding layer to be 5\n",
    "# hidden_size = 5 is arbitrarily chosen for easy visualization\n",
    "recurrent_layer = nn.LSTM(input_size=EMBEDDING_DIM,\n",
    "                          hidden_size=5,\n",
    "                          num_layers=1,\n",
    "                          dropout=0,\n",
    "                          batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical indices of bromoethane:\n",
      " tensor([[21, 21, 20]])\n",
      "\n",
      "Embedding:\n",
      " tensor([[[ 1.0882,  0.9560,  0.0367, -0.0861,  0.1151],\n",
      "         [ 1.0882,  0.9560,  0.0367, -0.0861,  0.1151],\n",
      "         [ 0.7384,  0.3497,  0.2501,  1.5561, -0.6903]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# get the numerical indices of bromoethane\n",
    "numerical_indices_smi_sample = torch.LongTensor([vocabulary.encode(tokenized_smi_sample).astype(int)])\n",
    "print(f\"Numerical indices of bromoethane:\\n {numerical_indices_smi_sample}\\n\")\n",
    "\n",
    "embedding = embedding_layer(numerical_indices_smi_sample)\n",
    "print(f\"Embedding:\\n {embedding}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validation / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45033, 2)\n",
      "Train data size: 36026\n",
      "Validation data size: 7205\n",
      "Test data size: 1802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Splitting the data into train and combined val/test sets\n",
    "train_data, val_test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the combined val/test set into separate val and test sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Printing the sizes of the resulting splits\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(val_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the NMT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('src/')\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from smiles_lstm.model.smiles_lstm import SmilesLSTM\n",
    "from smiles_lstm.model.smiles_trainer import SmilesTrainer\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, create_vocabulary\n",
    "from smiles_lstm.utils import load\n",
    "from smiles_lstm.utils.misc import suppress_warnings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 unique tokens in the vocabulary.\n",
      "\n",
      "Max length: 198.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train     = train_data.copy()\n",
    "test      = test_data.copy()\n",
    "valid     = val_data.copy()\n",
    "\n",
    "# create a vocabulary using all SMILES in df\n",
    "dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "dataset = np.unique(dataset).tolist()\n",
    "\n",
    "tokenizer = SMILESTokenizer()\n",
    "vocab     = create_vocabulary(smiles_list=dataset,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    canonical=False)\n",
    "\n",
    "MAX_LENGTH = max(len(v) for v in dataset)\n",
    "\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'Max length: {MAX_LENGTH}.\\n')\n",
    "# print(f'The unique tokens are: \\n{vocabulary.tokens()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'Br']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21., 21., 20.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = SMILESTokenizer()\n",
    "vocab = Vocabulary()\n",
    "\n",
    "smi_sample = 'CCBr'\n",
    "tokenized_smi_sample = tk.tokenize(smi_sample, with_begin_and_end=False)\n",
    "print(tokenized_smi_sample)\n",
    "vocabulary.encode(tokenized_smi_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for pad sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_sequence(tokenizer_array, desired_length):\n",
    "    padded_sequence = pad_sequences([tokenizer_array], maxlen=desired_length, padding='post')[0]\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [train, test, valid]:\n",
    "    for c in d.columns:\n",
    "        d[c] = d[c].apply(lambda x: tk.tokenize(x, with_begin_and_end=False))\n",
    "        d[c] = d[c].apply(lambda x: vocabulary.encode(x).astype(int))\n",
    "        d[c] = d[c].apply(lambda x: pad_sequence(x, MAX_LENGTH))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36026, 198) (36026, 198)\n"
     ]
    }
   ],
   "source": [
    "# Convert the source and target columns into numpy arrays\n",
    "trainX = np.array(train['source'].tolist())\n",
    "trainY = np.array(train['target'].tolist())\n",
    "\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 24s 19ms/step - loss: -551.8012 - accuracy: 0.0186\n",
      "Epoch 2/10\n",
      "1126/1126 [==============================] - 21s 19ms/step - loss: -1604.1986 - accuracy: 0.0016\n",
      "Epoch 3/10\n",
      "1126/1126 [==============================] - 22s 19ms/step - loss: -2647.5198 - accuracy: 0.0012\n",
      "Epoch 4/10\n",
      "1126/1126 [==============================] - 21s 18ms/step - loss: -3690.0530 - accuracy: 0.0012\n",
      "Epoch 5/10\n",
      "1126/1126 [==============================] - 21s 19ms/step - loss: -4732.2407 - accuracy: 0.0012\n",
      "Epoch 6/10\n",
      "1126/1126 [==============================] - 21s 19ms/step - loss: -5774.6333 - accuracy: 0.0012\n",
      "Epoch 7/10\n",
      "1126/1126 [==============================] - 21s 19ms/step - loss: -6816.7842 - accuracy: 0.0012\n",
      "Epoch 8/10\n",
      "1126/1126 [==============================] - 21s 19ms/step - loss: -7859.2627 - accuracy: 0.0012\n",
      "Epoch 9/10\n",
      "1126/1126 [==============================] - 21s 19ms/step - loss: -8901.6660 - accuracy: 0.0012\n",
      "Epoch 10/10\n",
      "1126/1126 [==============================] - 21s 19ms/step - loss: -9943.9619 - accuracy: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb63902de40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (trainX.shape[1], 1)  # Assuming you want to feed one feature at a time\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=input_shape))\n",
    "model.add(Dense(units=trainY.shape[1], activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1802, 198) (1802, 198)\n"
     ]
    }
   ],
   "source": [
    "# Convert the source and target columns into numpy arrays\n",
    "testX = np.array(test['source'].tolist())\n",
    "testY = np.array(test['target'].tolist())\n",
    "\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(units\u001b[39m=\u001b[39mNUM_EMBEDDING, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> 17\u001b[0m model \u001b[39m=\u001b[39m define_model()\n\u001b[1;32m     18\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36mdefine_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefine_model\u001b[39m():\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Define the LSTM model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     model \u001b[39m=\u001b[39m Sequential()\n\u001b[0;32m---> 12\u001b[0m     model\u001b[39m.\u001b[39madd(Embedding(input_dim\u001b[39m=\u001b[39mNUM_EMBEDDING, output_dim\u001b[39m=\u001b[39mEMBEDDING_DIM, input_length\u001b[39m=\u001b[39mMAX_LENGTH))\n\u001b[1;32m     13\u001b[0m     model\u001b[39m.\u001b[39madd(LSTM(units\u001b[39m=\u001b[39mHIDDEN_UNITS))\n\u001b[1;32m     14\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(units\u001b[39m=\u001b[39mNUM_EMBEDDING, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# construct an \"Embedding layer\"\n",
    "EMBEDDING_DIM = 5\n",
    "NUM_EMBEDDING = len(vocabulary) # 86\n",
    "MAX_LENGTH = max(len(v) for v in dataset)\n",
    "HIDDEN_UNITS = 10\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "def define_model():\n",
    "    # Define the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=NUM_EMBEDDING, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH))\n",
    "    model.add(LSTM(units=HIDDEN_UNITS))\n",
    "    model.add(Dense(units=NUM_EMBEDDING, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = define_model()\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the source and target columns into numpy arrays\n",
    "trainX = np.array(train['source'].tolist())\n",
    "trainY = np.array(train['target'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n      app.start()\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 530, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5144/4271738742.py\", line 1, in <module>\n      history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n      return self.compiled_loss(\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/losses.py\", line 284, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/losses.py\", line 2098, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/backend.py\", line 5633, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlabels must be 1-D, but got shape [512,198]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_29108]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/python-envs/main-bio/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n      app.start()\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/home/comex/.pyenv/versions/3.9.10/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 530, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5144/4271738742.py\", line 1, in <module>\n      history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n      return self.compiled_loss(\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/losses.py\", line 284, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/losses.py\", line 2098, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/comex/Desktop/python-envs/main-bio/lib/python3.9/site-packages/keras/backend.py\", line 5633, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlabels must be 1-D, but got shape [512,198]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_29108]"
     ]
    }
   ],
   "source": [
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                    epochs=30, batch_size=512, validation_split = 0.2, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
