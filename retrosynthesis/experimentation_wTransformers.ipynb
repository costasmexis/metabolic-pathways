{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "# ignore some deprecation warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load main data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...</td>\n",
       "      <td>CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1</td>\n",
       "      <td>COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C</td>\n",
       "      <td>CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O</td>\n",
       "      <td>CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fc1cc2c(NC3CCCCCC3)ncnc2cn1</td>\n",
       "      <td>Fc1cc2c(Cl)ncnc2cn1.NC1CCCCCC1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl)...   \n",
       "1      Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1    \n",
       "2    CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C    \n",
       "3            CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O    \n",
       "4                       Fc1cc2c(NC3CCCCCC3)ncnc2cn1    \n",
       "\n",
       "                                              target  \n",
       "0   CS(=O)(=O)OC[C@H]1CCC(=O)O1.Fc1ccc(Nc2ncnc3cc...  \n",
       "1      COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1  \n",
       "2   CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C.FC(F)(F)...  \n",
       "3    CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1  \n",
       "4                     Fc1cc2c(Cl)ncnc2cn1.NC1CCCCCC1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('retrosynthesis-all', header=None)\n",
    "df['source'] = df[0].apply(lambda x: x.split('>>')[0])\n",
    "df['target'] = df[0].apply(lambda x: x.split('>>')[1])\n",
    "df.drop(0, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Vocabulary: How is it generated?\n",
    "\n",
    "The model's Vocabulary handles the transformation of SMILES strings into a sequence of tokens. Tokens are the pre-defined lowest and indivisible unit of string text. In Natural Language Processing (NLP), tokens are typically defined on the word or character level. The level of tokenization dictates *what* the model can output, e.g., if tokenization on the character level is used, then the model outputs individual characters.\n",
    "\n",
    "For generative SMILES models, tokenization is performed on the character level where each token *loosely* maps to a unique atom type (brackets, \"(\" for example indicate branching and thus, do not map to an atom but rather gives connectivity information).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 unique tokens in the vocabulary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, Vocabulary, create_vocabulary\n",
    "\n",
    "tk = SMILESTokenizer()\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# create a vocabulary using all SMILES in df\n",
    "smiles_dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "smiles_dataset = np.unique(smiles_dataset).tolist()\n",
    "\n",
    "vocabulary = create_vocabulary(smiles_list=smiles_dataset, tokenizer=tk)\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes *how* the numerical representation of tokens are transformed into an input vector known as the embedding that will act as the input to the RNN.\n",
    "\n",
    "An Embedding Layer is essentially a look-up table. In the constructor above, `num_embeddings` refers to the Vocabulary size. \n",
    "\n",
    "`num_embeddings` denotes how many vectors to initialize. \n",
    "\n",
    "Since we have n unique tokens, we need _ different vectors: 1 for each unique token. This is why `num_embeddings` is _ in this example. `embedding_dim` denotes the dimension of the embedding vector. 5 is arbitrarily chosen here just for easy visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an \"Embedding layer\"\n",
    "EMBEDDING_DIM = 5\n",
    "NUM_EMBEDDING = len(vocabulary)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=NUM_EMBEDDING,\n",
    "                               embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# only 1 layer of LSTM cells is initialized here for the sake of illustration\n",
    "# input_size = 5 because we previously defined the \"embedding_dim\" of the Embedding layer to be 5\n",
    "# hidden_size = 5 is arbitrarily chosen for easy visualization\n",
    "recurrent_layer = nn.LSTM(input_size=EMBEDDING_DIM,\n",
    "                          hidden_size=5,\n",
    "                          num_layers=1,\n",
    "                          dropout=0,\n",
    "                          batch_first=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validation / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45033, 2)\n",
      "Train data size: 40529\n",
      "Validation data size: 3603\n",
      "Test data size: 901\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Splitting the data into train and combined val/test sets\n",
    "train_data, val_test_data = train_test_split(df, test_size=0.10, random_state=42)\n",
    "\n",
    "# Splitting the combined val/test set into separate val and test sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Printing the sizes of the resulting splits\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(val_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the NMT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('src/')\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from smiles_lstm.model.smiles_lstm import SmilesLSTM\n",
    "from smiles_lstm.model.smiles_trainer import SmilesTrainer\n",
    "from smiles_lstm.model.smiles_vocabulary import SMILESTokenizer, create_vocabulary\n",
    "from smiles_lstm.utils import load\n",
    "from smiles_lstm.utils.misc import suppress_warnings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 unique tokens in the vocabulary.\n",
      "\n",
      "Max length: 198.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train     = train_data.copy()\n",
    "test      = test_data.copy()\n",
    "valid     = val_data.copy()\n",
    "\n",
    "# create a vocabulary using all SMILES in df\n",
    "dataset = df['source'].unique().tolist()+ df['target'].unique().tolist()\n",
    "dataset = np.unique(dataset).tolist()\n",
    "\n",
    "tokenizer = SMILESTokenizer()\n",
    "vocab     = create_vocabulary(smiles_list=dataset,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    canonical=False)\n",
    "\n",
    "MAX_LENGTH = max(len(v) for v in dataset)\n",
    "\n",
    "print(f'There are {len(vocabulary)} unique tokens in the vocabulary.\\n')\n",
    "print(f'Max length: {MAX_LENGTH}.\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for pad sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_sequence(tokenizer_array, desired_length):\n",
    "    padded_sequence = pad_sequences([tokenizer_array], maxlen=desired_length, padding='post')[0]\n",
    "    return padded_sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and pad sequencing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [train, test, valid]:\n",
    "    for c in d.columns:\n",
    "        d[c] = d[c].apply(lambda x: tk.tokenize(x, with_begin_and_end=False))\n",
    "        d[c] = d[c].apply(lambda x: vocabulary.encode(x).astype(int))\n",
    "        d[c] = d[c].apply(lambda x: pad_sequence(x, MAX_LENGTH))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40529, 198) (40529, 198)\n"
     ]
    }
   ],
   "source": [
    "# Convert the source and target columns into numpy arrays\n",
    "trainX = np.array(train['source'].tolist())\n",
    "trainY = np.array(train['target'].tolist())\n",
    "\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = torch.LongTensor(trainX)\n",
    "trainY = torch.LongTensor(trainY)\n",
    "\n",
    "tensor = trainX[0]\n",
    "tensor = torch.reshape(tensor, (1, trainX.shape[1]))\n",
    "\n",
    "embedding = embedding_layer(tensor)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=NUM_EMBEDDING,\n",
    "                               embedding_dim=5)\n",
    "recurrent_layer = nn.LSTM(input_size=5,\n",
    "                          hidden_size=5,\n",
    "                          num_layers=1,\n",
    "                          dropout=0,\n",
    "                          batch_first=True)\n",
    "\n",
    "embedding = embedding_layer(tensor)\n",
    "# let's run the embedding through the recurrent layer\n",
    "rnn_output, (hidden_state, cell_state) = recurrent_layer(embedding)\n",
    "\n",
    "# initialize the linear layer\n",
    "# in_features = 5 as that is the hidden_size defined in the recurrent layer above\n",
    "# out_features = 20 as that is the size of the Vocabulary\n",
    "linear_layer = nn.Linear(in_features=5,\n",
    "                         out_features=NUM_EMBEDDING)\n",
    "\n",
    "linear_output = linear_layer(rnn_output)\n",
    "softmax = linear_output.softmax(dim=2)\n",
    "log_softmax = linear_output.log_softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 198])\n",
      "torch.Size([1, 198])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape)\n",
    "print(log_softmax.sum(dim=2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At time step 1, the generative model proposes ( as the most probable token and the correct token is O\n",
      "At time step 2, the generative model proposes [N-] as the most probable token and the correct token is =\n",
      "At time step 3, the generative model proposes [N-] as the most probable token and the correct token is C\n",
      "At time step 4, the generative model proposes 1 as the most probable token and the correct token is (\n",
      "At time step 5, the generative model proposes ( as the most probable token and the correct token is O\n",
      "At time step 6, the generative model proposes ( as the most probable token and the correct token is )\n",
      "At time step 7, the generative model proposes [N-] as the most probable token and the correct token is C\n",
      "At time step 8, the generative model proposes [N-] as the most probable token and the correct token is N\n",
      "At time step 9, the generative model proposes [N-] as the most probable token and the correct token is C\n",
      "At time step 10, the generative model proposes 1 as the most probable token and the correct token is (\n",
      "At time step 11, the generative model proposes [N-] as the most probable token and the correct token is =\n",
      "At time step 12, the generative model proposes 1 as the most probable token and the correct token is O\n",
      "At time step 13, the generative model proposes [Zn+] as the most probable token and the correct token is )\n",
      "At time step 14, the generative model proposes [N-] as the most probable token and the correct token is c\n",
      "At time step 15, the generative model proposes [N-] as the most probable token and the correct token is 1\n",
      "At time step 16, the generative model proposes [N-] as the most probable token and the correct token is c\n",
      "At time step 17, the generative model proposes 1 as the most probable token and the correct token is (\n",
      "At time step 18, the generative model proposes 1 as the most probable token and the correct token is O\n",
      "At time step 19, the generative model proposes C as the most probable token and the correct token is )\n",
      "At time step 20, the generative model proposes c as the most probable token and the correct token is n\n",
      "At time step 21, the generative model proposes [N-] as the most probable token and the correct token is c\n",
      "At time step 22, the generative model proposes 1 as the most probable token and the correct token is (\n",
      "At time step 23, the generative model proposes c as the most probable token and the correct token is -\n",
      "At time step 24, the generative model proposes c as the most probable token and the correct token is c\n",
      "At time step 25, the generative model proposes c as the most probable token and the correct token is 2\n",
      "At time step 26, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 27, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 28, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 29, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 30, the generative model proposes 1 as the most probable token and the correct token is (\n",
      "At time step 31, the generative model proposes 1 as the most probable token and the correct token is B\n",
      "At time step 32, the generative model proposes C as the most probable token and the correct token is r\n",
      "At time step 33, the generative model proposes [N-] as the most probable token and the correct token is )\n",
      "At time step 34, the generative model proposes C as the most probable token and the correct token is c\n",
      "At time step 35, the generative model proposes C as the most probable token and the correct token is 2\n",
      "At time step 36, the generative model proposes c as the most probable token and the correct token is )\n",
      "At time step 37, the generative model proposes ( as the most probable token and the correct token is n\n",
      "At time step 38, the generative model proposes [Zn+] as the most probable token and the correct token is (\n",
      "At time step 39, the generative model proposes [N-] as the most probable token and the correct token is C\n",
      "At time step 40, the generative model proposes C as the most probable token and the correct token is c\n",
      "At time step 41, the generative model proposes [N-] as the most probable token and the correct token is 2\n",
      "At time step 42, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 43, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 44, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 45, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 46, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 47, the generative model proposes [N-] as the most probable token and the correct token is 2\n",
      "At time step 48, the generative model proposes [OH-] as the most probable token and the correct token is )\n",
      "At time step 49, the generative model proposes [OH-] as the most probable token and the correct token is c\n",
      "At time step 50, the generative model proposes [N-] as the most probable token and the correct token is 1\n",
      "At time step 51, the generative model proposes [N-] as the most probable token and the correct token is =\n",
      "At time step 52, the generative model proposes [N-] as the most probable token and the correct token is O\n",
      "At time step 53, the generative model proposes [N-] as the most probable token and the correct token is  \n"
     ]
    }
   ],
   "source": [
    "most_probable_tokens = log_softmax.argmax(dim=2).flatten().tolist()\n",
    "\n",
    "# we now extract the max value in each tensor of the log-softmax output above and the corresponding token\n",
    "for idx, (correct_token, most_probable_token) in enumerate(zip(smiles, most_probable_tokens)):\n",
    "    print(f\"At time step {idx+1}, the generative model proposes {vocabulary.tokens()[most_probable_token]} as the most probable token and the correct token is {correct_token}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the model learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 2s 28ms/step - loss: 8103.3281 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 7555.0684 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 7301.1479 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 7189.0410 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 7121.8613 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 7074.5947 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 7038.3813 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 7009.2324 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 6985.0112 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 6964.4106 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad2056b9a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (trainX.shape[1], 1)  # Assuming you want to feed one feature at a time\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=NUM_EMBEDDING, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH))\n",
    "model.add(LSTM(units=128, input_shape=input_shape))\n",
    "model.add(Dense(units=trainY.shape[1], activation='log_softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.624979</td>\n",
       "      <td>-31.864088</td>\n",
       "      <td>-35.137905</td>\n",
       "      <td>-32.202782</td>\n",
       "      <td>-35.519382</td>\n",
       "      <td>-34.402882</td>\n",
       "      <td>-36.823887</td>\n",
       "      <td>-35.278248</td>\n",
       "      <td>-35.409874</td>\n",
       "      <td>-36.401161</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775615</td>\n",
       "      <td>-4.705916</td>\n",
       "      <td>-4.737956</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767376</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735643</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.624979</td>\n",
       "      <td>-31.864088</td>\n",
       "      <td>-35.137905</td>\n",
       "      <td>-32.202782</td>\n",
       "      <td>-35.519382</td>\n",
       "      <td>-34.402882</td>\n",
       "      <td>-36.823887</td>\n",
       "      <td>-35.278248</td>\n",
       "      <td>-35.409874</td>\n",
       "      <td>-36.401161</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775615</td>\n",
       "      <td>-4.705916</td>\n",
       "      <td>-4.737956</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767376</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735643</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.624979</td>\n",
       "      <td>-31.864088</td>\n",
       "      <td>-35.137905</td>\n",
       "      <td>-32.202782</td>\n",
       "      <td>-35.519382</td>\n",
       "      <td>-34.402882</td>\n",
       "      <td>-36.823887</td>\n",
       "      <td>-35.278248</td>\n",
       "      <td>-35.409874</td>\n",
       "      <td>-36.401161</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775615</td>\n",
       "      <td>-4.705916</td>\n",
       "      <td>-4.737956</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767376</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735643</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.624979</td>\n",
       "      <td>-31.864088</td>\n",
       "      <td>-35.137905</td>\n",
       "      <td>-32.202782</td>\n",
       "      <td>-35.519382</td>\n",
       "      <td>-34.402882</td>\n",
       "      <td>-36.823887</td>\n",
       "      <td>-35.278248</td>\n",
       "      <td>-35.409874</td>\n",
       "      <td>-36.401161</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775615</td>\n",
       "      <td>-4.705916</td>\n",
       "      <td>-4.737956</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767376</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735643</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.624979</td>\n",
       "      <td>-31.864088</td>\n",
       "      <td>-35.137905</td>\n",
       "      <td>-32.202782</td>\n",
       "      <td>-35.519382</td>\n",
       "      <td>-34.402882</td>\n",
       "      <td>-36.823887</td>\n",
       "      <td>-35.278248</td>\n",
       "      <td>-35.409874</td>\n",
       "      <td>-36.401161</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775615</td>\n",
       "      <td>-4.705916</td>\n",
       "      <td>-4.737956</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767376</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735643</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40524</th>\n",
       "      <td>-5.624981</td>\n",
       "      <td>-31.864100</td>\n",
       "      <td>-35.137920</td>\n",
       "      <td>-32.202793</td>\n",
       "      <td>-35.519398</td>\n",
       "      <td>-34.402893</td>\n",
       "      <td>-36.823902</td>\n",
       "      <td>-35.278263</td>\n",
       "      <td>-35.409885</td>\n",
       "      <td>-36.401176</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775614</td>\n",
       "      <td>-4.705914</td>\n",
       "      <td>-4.737955</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767377</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735642</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40525</th>\n",
       "      <td>-5.624981</td>\n",
       "      <td>-31.864100</td>\n",
       "      <td>-35.137920</td>\n",
       "      <td>-32.202793</td>\n",
       "      <td>-35.519398</td>\n",
       "      <td>-34.402893</td>\n",
       "      <td>-36.823902</td>\n",
       "      <td>-35.278263</td>\n",
       "      <td>-35.409885</td>\n",
       "      <td>-36.401176</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775614</td>\n",
       "      <td>-4.705914</td>\n",
       "      <td>-4.737955</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767377</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735642</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40526</th>\n",
       "      <td>-5.624981</td>\n",
       "      <td>-31.864100</td>\n",
       "      <td>-35.137920</td>\n",
       "      <td>-32.202793</td>\n",
       "      <td>-35.519398</td>\n",
       "      <td>-34.402893</td>\n",
       "      <td>-36.823902</td>\n",
       "      <td>-35.278263</td>\n",
       "      <td>-35.409885</td>\n",
       "      <td>-36.401176</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775614</td>\n",
       "      <td>-4.705914</td>\n",
       "      <td>-4.737955</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767377</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735642</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40527</th>\n",
       "      <td>-5.624981</td>\n",
       "      <td>-31.864100</td>\n",
       "      <td>-35.137920</td>\n",
       "      <td>-32.202793</td>\n",
       "      <td>-35.519398</td>\n",
       "      <td>-34.402893</td>\n",
       "      <td>-36.823902</td>\n",
       "      <td>-35.278263</td>\n",
       "      <td>-35.409885</td>\n",
       "      <td>-36.401176</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775614</td>\n",
       "      <td>-4.705914</td>\n",
       "      <td>-4.737955</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767377</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735642</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40528</th>\n",
       "      <td>-5.624981</td>\n",
       "      <td>-31.864100</td>\n",
       "      <td>-35.137920</td>\n",
       "      <td>-32.202793</td>\n",
       "      <td>-35.519398</td>\n",
       "      <td>-34.402893</td>\n",
       "      <td>-36.823902</td>\n",
       "      <td>-35.278263</td>\n",
       "      <td>-35.409885</td>\n",
       "      <td>-36.401176</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.857732</td>\n",
       "      <td>-4.775614</td>\n",
       "      <td>-4.705914</td>\n",
       "      <td>-4.737955</td>\n",
       "      <td>-4.692621</td>\n",
       "      <td>-4.749136</td>\n",
       "      <td>-4.767377</td>\n",
       "      <td>-4.761227</td>\n",
       "      <td>-4.735642</td>\n",
       "      <td>-4.782259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40529 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5    \\\n",
       "0     -5.624979 -31.864088 -35.137905 -32.202782 -35.519382 -34.402882   \n",
       "1     -5.624979 -31.864088 -35.137905 -32.202782 -35.519382 -34.402882   \n",
       "2     -5.624979 -31.864088 -35.137905 -32.202782 -35.519382 -34.402882   \n",
       "3     -5.624979 -31.864088 -35.137905 -32.202782 -35.519382 -34.402882   \n",
       "4     -5.624979 -31.864088 -35.137905 -32.202782 -35.519382 -34.402882   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "40524 -5.624981 -31.864100 -35.137920 -32.202793 -35.519398 -34.402893   \n",
       "40525 -5.624981 -31.864100 -35.137920 -32.202793 -35.519398 -34.402893   \n",
       "40526 -5.624981 -31.864100 -35.137920 -32.202793 -35.519398 -34.402893   \n",
       "40527 -5.624981 -31.864100 -35.137920 -32.202793 -35.519398 -34.402893   \n",
       "40528 -5.624981 -31.864100 -35.137920 -32.202793 -35.519398 -34.402893   \n",
       "\n",
       "             6          7          8          9    ...       188       189  \\\n",
       "0     -36.823887 -35.278248 -35.409874 -36.401161  ... -4.857732 -4.775615   \n",
       "1     -36.823887 -35.278248 -35.409874 -36.401161  ... -4.857732 -4.775615   \n",
       "2     -36.823887 -35.278248 -35.409874 -36.401161  ... -4.857732 -4.775615   \n",
       "3     -36.823887 -35.278248 -35.409874 -36.401161  ... -4.857732 -4.775615   \n",
       "4     -36.823887 -35.278248 -35.409874 -36.401161  ... -4.857732 -4.775615   \n",
       "...          ...        ...        ...        ...  ...       ...       ...   \n",
       "40524 -36.823902 -35.278263 -35.409885 -36.401176  ... -4.857732 -4.775614   \n",
       "40525 -36.823902 -35.278263 -35.409885 -36.401176  ... -4.857732 -4.775614   \n",
       "40526 -36.823902 -35.278263 -35.409885 -36.401176  ... -4.857732 -4.775614   \n",
       "40527 -36.823902 -35.278263 -35.409885 -36.401176  ... -4.857732 -4.775614   \n",
       "40528 -36.823902 -35.278263 -35.409885 -36.401176  ... -4.857732 -4.775614   \n",
       "\n",
       "            190       191       192       193       194       195       196  \\\n",
       "0     -4.705916 -4.737956 -4.692621 -4.749136 -4.767376 -4.761227 -4.735643   \n",
       "1     -4.705916 -4.737956 -4.692621 -4.749136 -4.767376 -4.761227 -4.735643   \n",
       "2     -4.705916 -4.737956 -4.692621 -4.749136 -4.767376 -4.761227 -4.735643   \n",
       "3     -4.705916 -4.737956 -4.692621 -4.749136 -4.767376 -4.761227 -4.735643   \n",
       "4     -4.705916 -4.737956 -4.692621 -4.749136 -4.767376 -4.761227 -4.735643   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "40524 -4.705914 -4.737955 -4.692621 -4.749136 -4.767377 -4.761227 -4.735642   \n",
       "40525 -4.705914 -4.737955 -4.692621 -4.749136 -4.767377 -4.761227 -4.735642   \n",
       "40526 -4.705914 -4.737955 -4.692621 -4.749136 -4.767377 -4.761227 -4.735642   \n",
       "40527 -4.705914 -4.737955 -4.692621 -4.749136 -4.767377 -4.761227 -4.735642   \n",
       "40528 -4.705914 -4.737955 -4.692621 -4.749136 -4.767377 -4.761227 -4.735642   \n",
       "\n",
       "            197  \n",
       "0     -4.782259  \n",
       "1     -4.782259  \n",
       "2     -4.782259  \n",
       "3     -4.782259  \n",
       "4     -4.782259  \n",
       "...         ...  \n",
       "40524 -4.782259  \n",
       "40525 -4.782259  \n",
       "40526 -4.782259  \n",
       "40527 -4.782259  \n",
       "40528 -4.782259  \n",
       "\n",
       "[40529 rows x 198 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(trainY)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' C=C(C)C(=O)Cl.CCCCCOC(CN)OCCCCC'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.untokenize(vocab.decode(testY[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99877197, 0.9998901 , 0.9999237 , ..., 0.01189647, 0.00572667,\n",
       "        0.00950348],\n",
       "       [0.99877197, 0.9998901 , 0.9999237 , ..., 0.01189647, 0.00572667,\n",
       "        0.00950348],\n",
       "       [0.99877197, 0.9998901 , 0.9999237 , ..., 0.01189647, 0.00572667,\n",
       "        0.00950348],\n",
       "       ...,\n",
       "       [0.99877197, 0.9998901 , 0.9999237 , ..., 0.01189647, 0.00572667,\n",
       "        0.00950348],\n",
       "       [0.99877197, 0.9998901 , 0.9999237 , ..., 0.01189647, 0.00572667,\n",
       "        0.00950348],\n",
       "       [0.99877197, 0.9998901 , 0.9999237 , ..., 0.01189647, 0.00572667,\n",
       "        0.00950348]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0.99877197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tk\u001b[39m.\u001b[39muntokenize(vocab\u001b[39m.\u001b[39;49mdecode(pred[\u001b[39m0\u001b[39;49m]))\n",
      "File \u001b[0;32m~/metabolic-pathways/retrosynthesis/src/smiles_lstm/model/smiles_vocabulary.py:73\u001b[0m, in \u001b[0;36mVocabulary.decode\u001b[0;34m(self, vocab_index)\u001b[0m\n\u001b[1;32m     71\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m vocab_index:\n\u001b[0;32m---> 73\u001b[0m     tokens\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m[idx])\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/metabolic-pathways/retrosynthesis/src/smiles_lstm/model/smiles_vocabulary.py:24\u001b[0m, in \u001b[0;36mVocabulary.__getitem__\u001b[0;34m(self, token_or_id)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, token_or_id : \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokens[token_or_id]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0.99877197"
     ]
    }
   ],
   "source": [
    "tk.untokenize(vocab.decode(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
